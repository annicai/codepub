{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Pub - Machine learning\n",
    "\n",
    "\n",
    " ![alt text](images/digits.png \"Handwritten digits 0-9\")\n",
    " \n",
    " \n",
    "## Handwritten digit recognition\n",
    "\n",
    "You will write code to classify images of handwritten digits using an ANN (artificial neural network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "First we will load our data. The data is stored as serialized Python objects and split into three sets - training data, validation data and test data.\n",
    "The training data will be used to train a machine learning model, the validation data to evaluate how well a certain model is doing and finally the test data will be used to evaluate the final model (This allows us to choose between different parameters in our learning algorithm, evaluate those with the validation data, choose the one which fares best on this data and still have a spare test set that have not been involved in the process of training/evaluating the models).\n",
    "\n",
    "Now load the data by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import cPickle as pickle\n",
    "\n",
    "f = gzip.open('data/alldata.pgz', 'rb')\n",
    "training_data, validation_data, test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of data\n",
    "\n",
    "Each dataset consists of a list of data points. One data point is a tuple, where the first entry is a 1x784 numpy array representing the image (a 28x28 image with grayscale values between 0-1 that has been reshaped to a 1d-array), and the second entry is a 1x10 array that represent the label (a digit between 0-9). \n",
    "\n",
    "The digit 3 will have a one as the 4th element and the rest zeros:\n",
    "\n",
    "    [[ 0.]\n",
    "     [ 0.]\n",
    "     [ 0.]\n",
    "     [ 1.]\n",
    "     [ 0.]\n",
    "     [ 0.]\n",
    "     [ 0.]\n",
    "     [ 0.]\n",
    "     [ 0.]\n",
    "     [ 0.]]\n",
    "\n",
    "Explore the data so that you understand the layout. \n",
    "Build a function that can display an a set of images given a dataset and the image indexes.\n",
    "Hint: The data need to be reshaped to a 28x28 representation again. Functions **reshape** and **matplotlib.pyplot.imshow** can be useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_images(data, idxs):\n",
    "    \"\"\" Display images \"\"\"\n",
    "    pass\n",
    "\n",
    "show_images(training_data, range(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Neural networks\n",
    "\n",
    "Now when you have familiarized yourself with the data, you will train a machine learning algorithm so that it can classify unseen images as digits.\n",
    "\n",
    "We will use a artificial neural network for this. The below picture shows a basic ANN.\n",
    "\n",
    " ![alt text](images/nn.png \"Example neural network\")\n",
    " \n",
    " This network consists of the input layer (yellow) with two nodes, one hidden layer (green) with three nodes and one output node (purple). This will thus take a 2-dimensional input and produce a 1-dimensional output.\n",
    "\n",
    "The weights and nodes in the network represent a function of the input variables $x_1, x_2 $. \n",
    "In each layer the inputs from the previous layer will be multiplied by the corresponding weights and summed together. This sum will go through an activation function and the result will be used as input to next layer.\n",
    "  \n",
    "The biological inspiration for ANNs is the neural networks found in our brains. We have synapses that connect neurons in a biological neural network and these connections grow stronger as more signals are sent between two neurons, when they \"fire\" together. A neuron will either fire or not based on its input signals.\n",
    " \n",
    "In our ANN we want to have a network where the correct neurons fire based on certain inputs. Firing or not is a binary function, so for instance we could say that if the sum $ \\sum_{i=1}^2 w_{i1}x_i $  is greater than some threshold we output 1 to the next layer, else 0.\n",
    " \n",
    "However, in practice this binary step function is rearly used. There are advantages, which will be seen later, using a differentiable function that have approximately this behaviour, such as the sigmoid function below.\n",
    "\n",
    " ![alt text](images/sigmoid.png \"Sigmoid function\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Sigmoid function\n",
    "\n",
    "We are going to use the sigmoid function as our activation function. The next step is for you to implement it.\n",
    "\n",
    "It looks like this:\n",
    "\n",
    "<h2 align=\"center\">S(z) = $\\frac{1}{(1+e^{-z})}$</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert np.abs(sigmoid(0.8) - .6899744811276125) < 0.00001\n",
    "assert np.abs(sigmoid(0.5) - .62245933120185459) < 0.00001\n",
    "assert np.abs(sigmoid(0.3) - .57444251681165903) < 0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple neural network\n",
    "\n",
    "The class below represents a simple neural network.\n",
    "Its has the same structure as the example ANN in the above picture. \n",
    "The weights are set such that the weights in the first layer are, $w_{11} = 1, w_{21} = 0.5, w_{12} = 2, w_{22} = 0.2, w_{13} = 0.4 w_{23} = 0.4$. The second layer weights are $w_1 = 1.5, w_2 = 4, w_3 = 0.2$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Simple_Network():\n",
    "    \"\"\"\n",
    "        This network has been initialized with static weights.\n",
    "        It has 2 inputs, 3 hidden nodes and 1 output, as in the picture above.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        # Weights are of size (# inputs, # hidden nodes)\n",
    "        # [1,0.5] are the weights coming in to the topmost hidden (purple) node, [2,0.2] to the middle one and\n",
    "        # [0.4,0.4] to the bottom one. \n",
    "        w1 = np.array([[1,0.5],[2,0.2],[0.4,0.4]]) \n",
    "        \n",
    "        # Weights in second layer are of size (# hidden nodes, # outputs)\n",
    "        w2 = np.array([[1.5, 4, 0.2]]) \n",
    "\n",
    "        self.weights = [w1, w2]\n",
    "        \n",
    "        # In general there will be as many bias terms as layers in the network\n",
    "        # (Biases will be explained in a later step and ignored in the first task)\n",
    "        self.biases = np.array([[0.5], [0.3]])\n",
    "\n",
    "    \n",
    "    def feedforward(self,a):\n",
    "        \"\"\"\n",
    "        Calculate the output based on input X.\n",
    "        This network only has one hidden layer, \n",
    "        but preferably the method should be genereal enough to work with any number of hidden layers.\n",
    "        \"\"\"\n",
    "        \n",
    "        pass\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the feedforward function\n",
    "\n",
    "Implement the feedforward function (ignore the biases for now, these will be explained later). \n",
    "The feedforward function should take a 2-dimensional input and propagate it through the network to calculate the output. \n",
    "\n",
    "The inputs should be multiplied with the weights of the first layer and summed together (which is exactly what the dot product does), then passed through the activation function (sigmoid). The result should be used as input to the next layer, where the input is again multiplied with the weights, summed and passed through the activation function.\n",
    "\n",
    "You should now implement the feedforward function for this simple network. \n",
    "Try to implement it for the general case of having an unknown number of hidden layers.\n",
    "\n",
    "You can test your implementation below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "simple_net = Simple_Network()\n",
    "output = simple_net.feedforward(np.array([2,1]))\n",
    "\n",
    "assert np.abs(output[0] - 0.99585138) < 0.001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feedforward function should be able to take multiple inputs and produce multiple outputs.\n",
    "Test this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simple_net = Simple_Network()\n",
    "output = simple_net.feedforward(np.array([[2.0,1.0], [3.0,3.0]]))\n",
    "\n",
    "assert np.abs(output[0][0] - 0.9962877) < 0.00001\n",
    "assert np.abs(output[0][1] - 0.99491348) < 0.00001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biases\n",
    "\n",
    "A bias in a neural network is a constant term that is independent of the input. It will also be trained, just like the other weights and represents a shift of the function, whereas the weights represent the steepness.\n",
    "It is somewhat equivalent to the m-term in a linear function, y = c*x + m.\n",
    "\n",
    "The activation of a node will take  $ \\sum_{i=1}^2 w_{i1}x_i + b $ as input if we include a bias.\n",
    "We will use one bias for each layer of the network. One other way of looking at the bias is as just adding an extra input node ($x_3$) in the input layer, which always has the value 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement feedforward to consider biases. The test below should pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simple_net = Simple_Network()\n",
    "output = simple_net.feedforward(np.array([[2.0,1.0], [3.0,3.0]]))\n",
    "\n",
    "assert np.abs(output[0][0] - 0.9973581) < 0.00001\n",
    "assert np.abs(output[0][1] - 0.99677828) < 0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "\n",
    "\n",
    "The example above represented a function, which yielded an output given an input, but its weights were static and it did not learn anything. \n",
    "\n",
    "In order to learn, it must use the training data to adjust the weights such that the ANN output matches the expected output as close as possible.\n",
    "\n",
    "In our case the input will be the 784 pixels of the image and the output a 1x10 array, where the final prediction will be decided by the index of the maximum activation. In the example below the 8:tn index has the highest activation (0.9). This represents the digit 8-1 = 7.\n",
    "\n",
    "output =  [0.2, 0.4, 0.3, 0.1, 0.2, 0.2, 0.1, **0.9**, 0.2, 0.1]\n",
    "\n",
    "When training the network we will compare the output to the optimal output, which will be [0, 0, 0, 0, 0, 0, 0, 1, 0, 0] for the number 7. The error for the above output is thus ([0, 0, 0, 0, 0, 0, 0, 1, 0, 0] - [0.2, 0.4, 0.3, 0.1, 0.2, 0.2, 0.1, 0.9, 0.2, 0.1]). The total error for our training data is then the sum of all the errors for the individual data points.\n",
    "\n",
    "One often aims to minimize the squared error term. The error is a function of the input data, its label and the weights in the network. The only thing that we are free to change are the weights. \n",
    "If we differentiate the error function with respect to the weights and choose to update the weights in the direction where the error decreases the most, the hope is that if we repeat this process we will slowly move towards a set of weights which defines the global minimum for the error term. \n",
    "This process is called **gradient descent**.\n",
    "\n",
    "There is the extra complication of having multiple layers, which makes this process less obvious, but it will work equally well as with one layer using the chain rule and what is called **backpropagation**. (There are however one problem with having too many layers and using backpropagation, which is called the vanishing gradient, but this is nothing that we need to worry about here)\n",
    "\n",
    "The exact details of backpropagation is beyond the scope of this introduction, but more information can be found here: http://neuralnetworksanddeeplearning.com/chap2.html.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### The derivative of the sigmoid function\n",
    "\n",
    "Since we will be differentiating the error function, we will have to implement a function that returns the derivate of the sigmoid function.\n",
    "\n",
    "This is:\n",
    "\n",
    "\n",
    "<h2 align=\"center\">S'(z) = $\\frac{S(z)}{(1-S(z))}$</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test that sigmoid_prime is correctly implemented\n",
    "\n",
    "assert np.abs(sigmoid_prime(0.5) - 0.235003712202) < 0.0001\n",
    "assert np.abs(sigmoid_prime(1.0) - 0.196611933241) < 0.0001\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the result \n",
    "\n",
    "We will use gradient descent to repetively update the weights to make the output of the network match the desired output for our training data. However, even if we can accurately classify all the examples in our training set, this does not imply that we will be able to classify new images that we have not seen before.\n",
    "\n",
    "If we would use a very large number of nodes in the hidden layer we could probably classify the training data 100% correctly. However the risk is that we would **overfit** our data, and learn to recognize exactly the training data and not the general apperances of the digits. We want the ANN to also be able to **generalize** well and be able to classify data that it has not yet seen. \n",
    "\n",
    "First, we need a we need a way to evaluate how well an ANN can classify unseen data. This is what the validation data set is for. \n",
    "You will create an evaluate() function that passes some validation/test data through the network (feedforward-function) and compares the output with the true labels. \n",
    "The function should count the number of inputs that give the correct result. Thus, we need to extract the actual prediction from the 1x10 output. Remember, for an output of [0.2, 0.4, 0.3, 0.1, 0.2, 0.2, 0.1, **0.9**, 0.2, 0.1] this will be 7, since the 8th index have the highest activation.\n",
    "\n",
    "Hint: np.argmax() returns the index for the largest element in an array\n",
    "\n",
    "Remember the structure of the data. Each data point consist of a tuple (input, label). The output when the input goes through the feedforward-process should be compared with the label. Then each sample is classified as either correct or not and all the correct samples are counted.\n",
    "\n",
    "Assume that the input parameter *net* has a feedforward-function just as the one you implemented earlier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(net, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load a pickled network that has not been trained. \n",
    "# All weights are random so this should not be good at classifying images\n",
    "# It will only be used to test your evaluation function\n",
    "\n",
    "f = gzip.open('data/randomnetwork.pgz', 'rb')\n",
    "random_network = cPickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test the evaluate function. If correctly implemented, 789 out of the 10000 samples should be correctly classified\n",
    "# This actually happen to be worse than if we were just guessing, in which case we would get 1000 correct on average\n",
    "\n",
    "counts = evaluate(random_network, validation_data)\n",
    "\n",
    "assert counts == 789"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network implementation\n",
    "\n",
    "Now we will more forward, away from the simple network we had before and to an ANN that can learn.\n",
    "You should complete the implementation of the Network class.\n",
    "The stochastic gradient descent (a faster gradient descent that only uses a subset of the data each time to update the weights) and backpropagation functions are given so that there will be time to play around with the network parameters.\n",
    "If you want to learn more about these and neural networks in general, http://neuralnetworksanddeeplearning.com, is a good source.\n",
    "\n",
    "The feedforward and evaluate-functions implemented above needs to the Network class.\n",
    "These need to be general and consider any number of hidden layers/input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This Network implementation can be found at http://neuralnetworksanddeeplearning.com/.\n",
    "More information about neural networks can be found there.\n",
    "~~~~~~~~~~\n",
    "\n",
    "A module to implement the stochastic gradient descent learning\n",
    "algorithm for a feedforward neural network.  Gradients are calculated\n",
    "using backpropagation.  Note that I have focused on making the code\n",
    "simple, easily readable, and easily modifiable.  It is not optimized,\n",
    "and omits many desirable features.\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print \"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test)\n",
    "            else:\n",
    "                print \"Epoch {0} complete\".format(j)\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "    \n",
    "    \n",
    "    # ----------------- Implement functions below ------------------------\n",
    "    \n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Train the network!\n",
    "\n",
    "Now it is time to train the network.\n",
    "\n",
    "There are a lot of parameters that can be tuned. \n",
    "\n",
    "*Epochs* is the number of training rounds used\n",
    "\n",
    "*Mini_batch_size* is a parameter for the weight update method we are using, stochastic gradient descent. Instead of using all data to update the weights, smaller random subsets will be used each iteration.\n",
    "\n",
    "*Eta* is a parameter that determines how much we will update the weights each iteration. Remember, we are updating the weights in the direction where the error is decreasing the most. If we choose a low value of *eta*, we will slowly move towards the minima. However, the risk is that we get stuck in a local minima. A larger value of *eta* explores the search space better, but we risk missing minima since we are sprinting past them too quickly, and we risk not converging at all.\n",
    "\n",
    "Start by calling SGD with the parameters below. Then experiment! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_hidden_nodes = 50\n",
    "epochs = 5 \n",
    "mini_batch_size = 10\n",
    "eta = 3.0\n",
    "\n",
    "net = Network([784, num_hidden_nodes, 10])\n",
    "\n",
    "# TODO: Call SGD to train the network using the above parameters. \n",
    "# Train using the training data and use the validation data as \"test_data\".\n",
    "\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning\n",
    "\n",
    "Try out different parameters.\n",
    "\n",
    "1. What happens if you increase the hidden node count? Is more always better?\n",
    "\n",
    "2. Try setting both the parameters training_data and test_data to the same dataset. This will mean that the evaluation will be done on the same data.\n",
    "   What happens now for different number of hidden nodes?\n",
    "    \n",
    "3. What happens if you change the other parameters? Play around and find a good combination!\n",
    "\n",
    "4. What happens if you increase/decrease the number of hidden *layers*?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Final accuracy\n",
    "\n",
    "Now you have evaluated different parameters using the validation data. \n",
    "Which parameters worked the best?\n",
    "Evaluate the final model using the test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
